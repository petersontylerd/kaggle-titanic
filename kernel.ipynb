{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:34.509604Z",
     "start_time": "2020-03-16T13:46:32.774066Z"
    },
    "code_folding": [
     26,
     34,
     42,
     52,
     83
    ]
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    IsolationForest,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import (\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    SGDRegressor,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    PolynomialFeatures,\n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    KBinsDiscretizer,\n",
    "    QuantileTransformer,\n",
    "    PowerTransformer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from category_encoders import (\n",
    "    WOEEncoder,\n",
    "    TargetEncoder,\n",
    "    CatBoostEncoder,\n",
    "    BinaryEncoder,\n",
    "    CountEncoder,\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import eif\n",
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "import squarify\n",
    "\n",
    "sys.path.append(f\"{os.environ['REPOS']}/mlmachine\")\n",
    "sys.path.append(f\"{os.environ['REPOS']}/prettierplot\")\n",
    "\n",
    "import mlmachine as mlm\n",
    "import mlmachine.data as data\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    KFoldEncoder,\n",
    "    GroupbyImputer,\n",
    "    PandasFeatureUnion,\n",
    "    DualTransformer,\n",
    ")\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "\n",
    "# experiment_path_root = \"/data/t1-tpeterso/repos/kaggle-titanic/experiments/titanic_survivorship_classification\"\n",
    "# experiment = \"210801185140\"\n",
    "\n",
    "# # reload objects\n",
    "# machine = pickle.load(open(os.path.join(experiment_path_root, experiment, \"machine\", \"machine.pkl\"), 'rb'))\n",
    "# # impute_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"impute_pipe.pkl\"), 'rb'))\n",
    "# # polynomial_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"polynomial_pipe.pkl\"), 'rb'))\n",
    "# # encode_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"encode_pipe.pkl\"), 'rb'))\n",
    "# # target_encode_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"target_encode_pipe.pkl\"), 'rb'))\n",
    "# # skew_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"skew_pipe.pkl\"), 'rb'))\n",
    "# scale_pipe = pickle.load(open(os.path.join(experiment_path_root, experiment, \"transformers\", \"scale_pipe.pkl\"), 'rb'))\n",
    "# fs = pickle.load(open(os.path.join(experiment_path_root, experiment, \"feature_selection\", \"FeatureSelector.pkl\"), 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:34.534602Z",
     "start_time": "2020-03-16T13:46:34.510605Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "df_train, df_valid = data.titanic()\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(df_train.shape))\n",
    "print(\"Validation data dimensions: {}\".format(df_valid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:34.562609Z",
     "start_time": "2020-03-16T13:46:34.535607Z"
    }
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "df_train.info()\n",
    "display(df_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:34.580614Z",
     "start_time": "2020-03-16T13:46:34.563609Z"
    }
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "df_train.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create machine object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:34.609616Z",
     "start_time": "2020-03-16T13:46:34.581613Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "df_train, df_valid = mlm.train_test_df_compile(data=df_train, target_col=\"Survived\")\n",
    "\n",
    "# Load dataset into mlmachine\n",
    "ordinal_encodings = {\n",
    "        \"Pclass\": [1, 2, 3], # Pclass\n",
    "    }\n",
    "\n",
    "machine = mlm.Machine(\n",
    "    experiment_name=\"titanic_survivorship_classification\",\n",
    "    training_dataset=df_train,\n",
    "    validation_dataset=df_valid,    \n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\", \"Ticket\", \"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\",\"Name\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings = ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review mlm dtypes\n",
    "machine.training_features.mlm_dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Category feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:35.638772Z",
     "start_time": "2020-03-16T13:46:34.628619Z"
    }
   },
   "outputs": [],
   "source": [
    "# category features\n",
    "for feature in machine.training_features.mlm_dtypes[\"category\"]:\n",
    "    machine.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        level_count_cap=10,\n",
    "        legend_labels=[\"Died\",\"Survived\"],\n",
    "        chart_scale=15,\n",
    "        training_data=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Count feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:36.497901Z",
     "start_time": "2020-03-16T13:46:35.640770Z"
    }
   },
   "outputs": [],
   "source": [
    "# number features\n",
    "for feature in machine.training_features.mlm_dtypes[\"count\"]:\n",
    "    machine.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        level_count_cap=10,\n",
    "        legend_labels=[\"Died\",\"Survived\"],\n",
    "        chart_scale=15\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:37.703090Z",
     "start_time": "2020-03-16T13:46:36.498897Z"
    }
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "for feature in machine.training_features.mlm_dtypes[\"continuous\"]:\n",
    "    machine.eda_cat_target_num_feat(\n",
    "        feature=feature,\n",
    "        outliers_out_of_scope=5,\n",
    "        legend_labels=[\"Died\",\"Survived\"],\n",
    "        chart_scale=12\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:37.703090Z",
     "start_time": "2020-03-16T13:46:36.498897Z"
    }
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "machine.eda(save_plots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:37.928122Z",
     "start_time": "2020-03-16T13:46:37.704088Z"
    }
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:38.081148Z",
     "start_time": "2020-03-16T13:46:37.929118Z"
    }
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_scale=10)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap_target(\n",
    "    df=machine.training_features,\n",
    "    target=machine.training_target,\n",
    "    thresh=0.01,\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:39.013287Z",
     "start_time": "2020-03-16T13:46:38.082145Z"
    }
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chart_scale=15)\n",
    "p.pair_plot(\n",
    "    df=machine.training_features[[\"Age\",\"Fare\"]],\n",
    "    target=machine.training_target,\n",
    "    diag_kind=\"auto\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:39.184312Z",
     "start_time": "2020-03-16T13:46:39.014286Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Survivorship, embark location by passenger class\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    x=\"Embarked\",\n",
    "    y=machine.training_target.name,\n",
    "    split=\"Pclass\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:39.334335Z",
     "start_time": "2020-03-16T13:46:39.185308Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Survivorship, passenger class by gender\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    x=\"Pclass\",\n",
    "    y=machine.training_target.name,\n",
    "    split=\"Sex\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:39.480359Z",
     "start_time": "2020-03-16T13:46:39.335336Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "ax = p.make_canvas(title=\"Survivorship,embark location by gender\", y_shift=0.7)\n",
    "p.facet_two_cat_bar(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    x=\"Embarked\",\n",
    "    y=machine.training_target.name,\n",
    "    split=\"Sex\",\n",
    "    y_units=\"ff\",\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:39.854421Z",
     "start_time": "2020-03-16T13:46:39.481362Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_two_cat_point(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    x=\"Sex\",\n",
    "    y=machine.training_target.name,\n",
    "    split=\"Pclass\",\n",
    "    cat_col=\"Embarked\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "    legend_labels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:40.137458Z",
     "start_time": "2020-03-16T13:46:39.855425Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_two_cat_point(\n",
    "    df=machine.recombine_data(training_data=True).dropna(subset=[\"Embarked\"]),\n",
    "    x=\"Embarked\",\n",
    "    y=machine.training_target.name,\n",
    "    split=\"Pclass\",\n",
    "    cat_col=\"Sex\",\n",
    "    aspect=1.0,\n",
    "    height=5,\n",
    "    bbox=(1.5, 0.8),\n",
    "    legend_labels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:41.339644Z",
     "start_time": "2020-03-16T13:46:40.138458Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.facet_cat_num_hist(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    split=machine.training_target.name,\n",
    "    legend_labels=[\"Died\", \"Lived\"],\n",
    "    cat_row=\"Sex\",\n",
    "    cat_col=\"Embarked\",\n",
    "    num_col=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:42.304798Z",
     "start_time": "2020-03-16T13:46:41.340645Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot(chart_scale=15)\n",
    "p.facet_cat_num_scatter(\n",
    "    df=machine.recombine_data(training_data=True),\n",
    "    split=machine.training_target.name,\n",
    "    legend_labels=[\"Died\", \"Lived\"],\n",
    "    cat_row=\"Sex\",\n",
    "    cat_col=\"Embarked\",\n",
    "    x=\"Fare\",\n",
    "    y=\"Age\",\n",
    "    bbox=(1.9, 1.0),\n",
    "    height=4,\n",
    "    aspect=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:42.311794Z",
     "start_time": "2020-03-16T13:46:42.305797Z"
    }
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(machine.training_target).value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:42.538827Z",
     "start_time": "2020-03-16T13:46:42.312791Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "machine.eda_missing_summary(training_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:42.878880Z",
     "start_time": "2020-03-16T13:46:42.539826Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(machine.training_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:43.337950Z",
     "start_time": "2020-03-16T13:46:42.881880Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(machine.training_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:43.676002Z",
     "start_time": "2020-03-16T13:46:43.339951Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(machine.training_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:43.886034Z",
     "start_time": "2020-03-16T13:46:43.677002Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(machine.training_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:44.091069Z",
     "start_time": "2020-03-16T13:46:43.887032Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "machine.eda_missing_summary(training_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:44.433118Z",
     "start_time": "2020-03-16T13:46:44.092068Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno matrix\n",
    "msno.matrix(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:44.883188Z",
     "start_time": "2020-03-16T13:46:44.434116Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno bar\n",
    "msno.bar(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.128226Z",
     "start_time": "2020-03-16T13:46:44.884188Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno heatmap\n",
    "msno.heatmap(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.339262Z",
     "start_time": "2020-03-16T13:46:45.129230Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingno dendrogram\n",
    "msno.dendrogram(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training vs. validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.348264Z",
     "start_time": "2020-03-16T13:46:45.340269Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "machine.missing_column_compare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.394268Z",
     "start_time": "2020-03-16T13:46:45.349259Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Age\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\")\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "#     (\"cabin\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=[\"Cabin\"]),\n",
    "#         PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "#     )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "impute_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"impute_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(impute_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = impute_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = impute_pipe.transform(machine.validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.402270Z",
     "start_time": "2020-03-16T13:46:45.395268Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "machine.eda_missing_summary(training_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.415274Z",
     "start_time": "2020-03-16T13:46:45.403271Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "machine.eda_missing_summary(training_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.431272Z",
     "start_time": "2020-03-16T13:46:45.416271Z"
    }
   },
   "outputs": [],
   "source": [
    "### training data\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in machine.training_features[\"Name\"]]\n",
    "machine.training_features[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=machine.training_features.index,\n",
    "    dtype=\"object\",\n",
    ")\n",
    "machine.training_features[\"Title\"] = machine.training_features[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "machine.training_features[\"Title\"] = machine.training_features[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "machine.training_features[\"Title\"] = machine.training_features[\"Title\"].astype(\"category\")\n",
    "\n",
    "# # distill cabin feature\n",
    "# machine.training_features[\"CabinQuarter\"] = pd.Series(\n",
    "#     [i[0] if not pd.isnull(i) else \"X\" for i in machine.training_features[\"Cabin\"]],\n",
    "#     index=machine.training_features.index,\n",
    "#     dtype=\"category\",\n",
    "# )\n",
    "\n",
    "# family size features and binning\n",
    "machine.training_features[\"FamilySize\"] = machine.training_features[\"SibSp\"] + machine.training_features[\"Parch\"] + 1\n",
    "machine.training_features[\"FamilySize\"] = machine.training_features[\"FamilySize\"].astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.445275Z",
     "start_time": "2020-03-16T13:46:45.432271Z"
    }
   },
   "outputs": [],
   "source": [
    "### validation data\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(\",\")[1].split(\".\")[0].strip() for i in machine.validation_features[\"Name\"]]\n",
    "machine.validation_features[\"Title\"] = pd.Series(\n",
    "    title,\n",
    "    index=machine.validation_features.index,\n",
    "    dtype=\"object\",\n",
    ")\n",
    "machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].replace(\n",
    "    [\n",
    "        \"Lady\",\n",
    "        \"the Countess\",\n",
    "        \"Countess\",\n",
    "        \"Capt\",\n",
    "        \"Col\",\n",
    "        \"Don\",\n",
    "        \"Dr\",\n",
    "        \"Major\",\n",
    "        \"Rev\",\n",
    "        \"Sir\",\n",
    "        \"Jonkheer\",\n",
    "        \"Dona\",\n",
    "    ],\n",
    "    \"Rare\",\n",
    ")\n",
    "machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].map(\n",
    "    {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    ")\n",
    "machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].astype(\"category\")\n",
    "\n",
    "# # distill cabin feature\n",
    "# machine.validation_features[\"CabinQuarter\"] = pd.Series(\n",
    "#     [i[0] if not pd.isnull(i) else \"X\" for i in machine.validation_features[\"Cabin\"]],\n",
    "#     index=machine.validation_features.index,\n",
    "#     dtype=\"category\",\n",
    "# )\n",
    "\n",
    "# additional features\n",
    "machine.validation_features[\"FamilySize\"] = machine.validation_features[\"SibSp\"] + machine.validation_features[\"Parch\"] + 1\n",
    "machine.validation_features[\"FamilySize\"] = machine.validation_features[\"FamilySize\"].astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.490281Z",
     "start_time": "2020-03-16T13:46:45.446272Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform pipe\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasTransformer(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"], exclude_columns=[\"Name\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "polynomial_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"polynomial_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(polynomial_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = polynomial_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = polynomial_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.501284Z",
     "start_time": "2020-03-16T13:46:45.491281Z"
    }
   },
   "outputs": [],
   "source": [
    "### training data\n",
    "# counts of unique values in training data string columns\n",
    "machine.training_features[machine.training_features.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.510284Z",
     "start_time": "2020-03-16T13:46:45.502283Z"
    }
   },
   "outputs": [],
   "source": [
    "### train data\n",
    "# print unique values in each category columns\n",
    "machine.unique_category_levels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.519286Z",
     "start_time": "2020-03-16T13:46:45.511283Z"
    }
   },
   "outputs": [],
   "source": [
    "### validation data\n",
    "# counts of unique values in validation data string columns\n",
    "machine.validation_features[machine.training_features.mlm_dtypes[\"category\"]].apply(pd.Series.nunique, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.533288Z",
     "start_time": "2020-03-16T13:46:45.520286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### validation data\n",
    "# print unique values in each category columns\n",
    "machine.unique_category_levels(training_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.542289Z",
     "start_time": "2020-03-16T13:46:45.534301Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "machine.compare_train_valid_levels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:45.691312Z",
     "start_time": "2020-03-16T13:46:45.543290Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# encode pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"nominal\"]),\n",
    "        PandasTransformer(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "#     (\"bin\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"continuous\"]),\n",
    "#         PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "#     )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=machine.training_features.mlm_dtypes[\"nominal\"] + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "encode_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = encode_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:52.463361Z",
     "start_time": "2020-03-16T13:46:45.692312Z"
    }
   },
   "outputs": [],
   "source": [
    "# target encoding pipe\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=machine.training_target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "#     (\"woe\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "#         KFoldEncoder(\n",
    "#             target=machine.training_target,\n",
    "#             cv=KFold(n_splits=5, shuffle=False),\n",
    "#             encoder=WOEEncoder,\n",
    "#         ),\n",
    "#     )),\n",
    "#     (\"catboost\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "#         KFoldEncoder(\n",
    "#             target=machine.training_target,\n",
    "#             cv=KFold(n_splits=5, shuffle=False),\n",
    "#             encoder=CatBoostEncoder,\n",
    "#         ),\n",
    "#     )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "target_encode_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"target_encode_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(target_encode_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = target_encode_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = target_encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Skew correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:52.726394Z",
     "start_time": "2020-03-16T13:46:52.464353Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training data\n",
    "# evaluate skew of number features\n",
    "machine.skew_summary(columns=machine.training_features.mlm_dtypes[\"continuous\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:52.893424Z",
     "start_time": "2020-03-16T13:46:52.727394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### validation data\n",
    "# evaluate skew of number features\n",
    "machine.skew_summary(training_data=False, columns=machine.training_features.mlm_dtypes[\"continuous\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:53.916575Z",
     "start_time": "2020-03-16T13:46:52.894416Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correction pipeline\n",
    "skew_pipe = PandasFeatureUnion([\n",
    "    (\"skew\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "skew_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"skew_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(skew_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = skew_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = skew_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:54.032595Z",
     "start_time": "2020-03-16T13:46:53.917575Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "scale_pipe = PandasFeatureUnion([\n",
    "    (\"scale\", make_pipeline(\n",
    "        DataFrameSelector(),\n",
    "        PandasTransformer(RobustScaler())\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit & save objects\n",
    "scale_pipe.fit(machine.training_features)\n",
    "with open(os.path.join(machine.current_experiment_dir, \"transformers\", \"scale_pipe.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(scale_pipe, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# transform datasets\n",
    "machine.training_features = scale_pipe.fit_transform(machine.training_features)\n",
    "machine.validation_features = scale_pipe.transform(machine.validation_features)\n",
    "\n",
    "machine.update_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:54.221623Z",
     "start_time": "2020-03-16T13:46:54.033595Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",machine.OutlierIQR(\n",
    "                outlier_count=10,\n",
    "                iqr_step=1.5,\n",
    "                features=machine.training_features.mlm_dtypes[\"continuous\"],\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "machine.training_features = train_pipe.transform(machine.training_features)\n",
    "\n",
    "# capture outliers\n",
    "iqr_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(iqr_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:54.750704Z",
     "start_time": "2020-03-16T13:46:54.222623Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = IsolationForest(\n",
    "#     behaviour=\"new\",\n",
    "    max_samples=machine.training_features.shape[0],\n",
    "    random_state=0,\n",
    "    contamination=0.01,\n",
    ")\n",
    "clf.fit(machine.training_features[machine.training_features.columns])\n",
    "preds = clf.predict(machine.training_features[machine.training_features.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "if_outliers = np.array(machine.training_features[mask].index)\n",
    "print(if_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:59.198388Z",
     "start_time": "2020-03-16T13:46:54.751702Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "train_pipe = Pipeline([\n",
    "    (\"outlier\",machine.ExtendedIsoForest(\n",
    "                columns=machine.training_features.mlm_dtypes[\"continuous\"],\n",
    "                n_trees=100,\n",
    "                sample_size=256,\n",
    "                extension_level=1,\n",
    "                anomalies_ratio=0.03,\n",
    "                drop_outliers=False,))\n",
    "    ])\n",
    "machine.training_features = train_pipe.transform(machine.training_features)\n",
    "\n",
    "# capture outliers\n",
    "eif_outliers = np.array(sorted(train_pipe.named_steps[\"outlier\"].outliers))\n",
    "print(eif_outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:59.204388Z",
     "start_time": "2020-03-16T13:46:59.199386Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqr_outliers, if_outliers, eif_outliers))\n",
    "# outliers = reduce(np.intersect1d, (if_outliers, eif_outliers))\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:59.225392Z",
     "start_time": "2020-03-16T13:46:59.205390Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlier_summary = machine.outlier_summary(iqr_outliers=iqr_outliers,\n",
    "                             if_outliers=if_outliers,\n",
    "                             eif_outliers=eif_outliers\n",
    "                            )\n",
    "outlier_summary[outlier_summary[\"count\"] >= 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T13:46:59.233392Z",
     "start_time": "2020-03-16T13:46:59.226391Z"
    }
   },
   "outputs": [],
   "source": [
    "# # remove outlers from predictors and response\n",
    "# outliers = np.array([258, 305, 438, 679, 737, 745])\n",
    "# machine.training_features = machine.training_features.drop(outliers)\n",
    "# machine.training_target = machine.training_target.drop(index=outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Additional exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:00.719622Z",
     "start_time": "2020-03-16T13:46:59.234392Z"
    }
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plot_orientation='tall',chart_scale=15)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap_target(\n",
    "    df=machine.training_features,\n",
    "    target=machine.training_target,\n",
    "    thresh=0.3,\n",
    "    annot=True,\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:07.808718Z",
     "start_time": "2020-03-16T13:47:00.720621Z"
    }
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot(chart_scale=25)\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(df=machine.training_features, annot=False, ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save machine object\n",
    "with open(os.path.join(machine.current_experiment_dir, \"machine\", \"machine.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(machine, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-16T04:18:30.002723Z",
     "start_time": "2020-03-15T21:53:05.300999Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate feature importance summary\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "lgb2 = LGBMClassifier(max_depth=2, n_estimators=500)\n",
    "lgb3 = LGBMClassifier(max_depth=3, n_estimators=500)\n",
    "lgb4 = LGBMClassifier(max_depth=4, n_estimators=500)\n",
    "\n",
    "xgb2 = XGBClassifier(max_depth=2, n_estimators=500)\n",
    "xgb3 = XGBClassifier(max_depth=3, n_estimators=500)\n",
    "xgb4 = XGBClassifier(max_depth=4, n_estimators=500)\n",
    "\n",
    "rf2 = RandomForestClassifier(max_depth=2, n_estimators=500)\n",
    "rf3 = RandomForestClassifier(max_depth=3, n_estimators=500)\n",
    "rf4 = RandomForestClassifier(max_depth=4, n_estimators=500)\n",
    "\n",
    "estimators = [\n",
    "    SVC,\n",
    "    LGBMClassifier,\n",
    "    LogisticRegression,\n",
    "    XGBClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    KNeighborsClassifier,\n",
    "#     knn10,\n",
    "#     lgb2,\n",
    "#     lgb3,\n",
    "#     lgb4,\n",
    "#     xgb2,\n",
    "#     xgb3,\n",
    "#     xgb4,\n",
    "#     rf2,\n",
    "#     rf3,\n",
    "#     rf4,\n",
    "]\n",
    "\n",
    "fs = machine.FeatureSelector(\n",
    "    training_features=machine.training_features,\n",
    "    training_target=machine.training_target,\n",
    "    validation_features=machine.validation_features,\n",
    "    validation_target=machine.validation_target,\n",
    "    estimators=estimators,\n",
    "    experiment_dir=machine.current_experiment_dir,\n",
    ")\n",
    "\n",
    "fs.feature_selector_suite(\n",
    "    sequential_scoring=[\"roc_auc\"],\n",
    "#     sequential_scoring=[\"accuracy\",\"precision\",\"recall\",\"roc_auc\"],\n",
    "    n_jobs=4,\n",
    "    save_to_csv=True,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:29:02.475327Z",
     "start_time": "2020-02-02T04:17:27.378974Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate cross-validation performance\n",
    "fs.run_cross_val(\n",
    "    estimators=estimators,\n",
    "    scoring=[\"roc_auc\"],\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=2,\n",
    "    save_to_csv=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs.cv_summary[fs.cv_summary[\"estimator\"] == \"LGBMClassifier\"].sort_values(\"validation score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:29:41.954030Z",
     "start_time": "2020-02-02T04:29:36.992053Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize CV performance for diminishing feature set\n",
    "fs.plot_results(\n",
    "    scoring=\"roc_auc_score\",\n",
    "    title_scale=0.8,\n",
    "    save_plots=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:30:03.098743Z",
     "start_time": "2020-02-02T04:30:03.026958Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "fs.create_cross_val_features_df(scoring=\"roc_auc_score\")\n",
    "# fs.cross_val_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:30:05.091318Z",
     "start_time": "2020-02-02T04:30:05.082706Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "fs.create_cross_val_features_dict(scoring=\"roc_auc_score\")\n",
    "fs.cross_val_features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature selector\n",
    "with open(os.path.join(machine.current_experiment_dir, \"feature_selection\", \"FeatureSelector.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(fs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-03-15T21:51:13.876489Z",
     "start_time": "2020-03-15T21:51:06.773796Z"
    },
    "code_folding": [
     5,
     9,
     22,
     35,
     65,
     71,
     93,
     107,
     113,
     150,
     167,
     193,
     230
    ]
   },
   "outputs": [],
   "source": [
    "# #################################################################################\n",
    "# # import training data\n",
    "# df_train, df_valid = data.titanic()\n",
    "\n",
    "# #\n",
    "# df_train, df_valid = mlm.train_test_df_compile(data=df_train, target_col=\"Survived\")\n",
    "\n",
    "\n",
    "# # Load training data into mlmachine\n",
    "# ordinal_encodings = {\n",
    "#         \"Pclass\": [1, 2, 3], # Pclass\n",
    "#     }\n",
    "\n",
    "# machine = mlm.Machine(\n",
    "#     experiment_name=\"titanic_survivorship_classification\",\n",
    "#     training_dataset=df_train,\n",
    "#     validation_dataset=df_valid,    \n",
    "#     target=\"Survived\",\n",
    "#     remove_features=[\"PassengerId\", \"Ticket\"],\n",
    "#     identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "#     identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "#     identify_as_nominal=[\"Cabin\",\"Embarked\",\"Name\"],\n",
    "#     identify_as_ordinal=[\"Pclass\"],\n",
    "#     ordinal_encodings = ordinal_encodings,\n",
    "#     is_classification=True,\n",
    "# )\n",
    "\n",
    "# #################################################################################\n",
    "# # impute pipeline\n",
    "# impute_pipe = PandasFeatureUnion([\n",
    "#     (\"age\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=[\"Age\",\"Pclass\"]),\n",
    "#         GroupbyImputer(null_column=\"Age\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "#     )),\n",
    "#     (\"fare\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "#         GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\")\n",
    "#     )),\n",
    "#     (\"embarked\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "#         PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "#     )),\n",
    "#     (\"cabin\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=[\"Cabin\"]),\n",
    "#         PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "#     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\",\"Cabin\"])\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = impute_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = impute_pipe.transform(machine.validation_features)\n",
    "\n",
    "# #################################################################################\n",
    "# # feature engineering - training\n",
    "\n",
    "# # parse titles to learn gender, and identify rare titles which may convey status\n",
    "# title = [i.split(\",\")[1].split(\".\")[0].strip() for i in machine.training_features[\"Name\"]]\n",
    "# machine.training_features[\"Title\"] = pd.Series(\n",
    "#     title,\n",
    "#     index=machine.training_features.index,\n",
    "#     dtype=\"object\",\n",
    "# )\n",
    "# machine.training_features[\"Title\"] = machine.training_features[\"Title\"].replace(\n",
    "#     [\n",
    "#         \"Lady\",\n",
    "#         \"the Countess\",\n",
    "#         \"Countess\",\n",
    "#         \"Capt\",\n",
    "#         \"Col\",\n",
    "#         \"Don\",\n",
    "#         \"Dr\",\n",
    "#         \"Major\",\n",
    "#         \"Rev\",\n",
    "#         \"Sir\",\n",
    "#         \"Jonkheer\",\n",
    "#         \"Dona\",\n",
    "#     ],\n",
    "#     \"Rare\",\n",
    "# )\n",
    "# machine.training_features[\"Title\"] = machine.training_features[\"Title\"].map(\n",
    "#     {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    "# )\n",
    "# machine.training_features[\"Title\"] = machine.training_features[\"Title\"].astype(\"category\")\n",
    "\n",
    "# # distill cabin feature\n",
    "# machine.training_features[\"CabinQuarter\"] = pd.Series(\n",
    "#     [i[0] if not pd.isnull(i) else \"X\" for i in machine.training_features[\"Cabin\"]],\n",
    "#     index=machine.training_features.index,\n",
    "#     dtype=\"category\",\n",
    "# )\n",
    "\n",
    "# # family size features\n",
    "# machine.training_features[\"FamilySize\"] = pd.to_numeric(machine.training_features[\"SibSp\"]) + pd.to_numeric(machine.training_features[\"Parch\"]) + 1\n",
    "\n",
    "# #################################################################################\n",
    "# # feature engineering - validation\n",
    "\n",
    "# # parse titles to learn gender, and identify rare titles which may convey status\n",
    "# title = [i.split(\",\")[1].split(\".\")[0].strip() for i in machine.validation_features[\"Name\"]]\n",
    "# machine.validation_features[\"Title\"] = pd.Series(\n",
    "#     title,\n",
    "#     index=machine.validation_features.index,\n",
    "#     dtype=\"object\"\n",
    "# )\n",
    "# machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].replace(\n",
    "#     [\n",
    "#         \"Lady\",\n",
    "#         \"the Countess\",\n",
    "#         \"Countess\",\n",
    "#         \"Capt\",\n",
    "#         \"Col\",\n",
    "#         \"Don\",\n",
    "#         \"Dr\",\n",
    "#         \"Major\",\n",
    "#         \"Rev\",\n",
    "#         \"Sir\",\n",
    "#         \"Jonkheer\",\n",
    "#         \"Dona\",\n",
    "#     ],\n",
    "#     \"Rare\",\n",
    "# )\n",
    "# machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].map(\n",
    "#     {\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3}\n",
    "# )\n",
    "# machine.validation_features[\"Title\"] = machine.validation_features[\"Title\"].astype(\"category\")\n",
    "\n",
    "# # distill cabin feature\n",
    "# machine.validation_features[\"CabinQuarter\"] = pd.Series(\n",
    "#     [i[0] if not pd.isnull(i) else \"X\" for i in machine.validation_features[\"Cabin\"]],\n",
    "#     index=machine.validation_features.index,\n",
    "#     dtype=\"category\",\n",
    "# )\n",
    "\n",
    "# # additional features\n",
    "# machine.validation_features[\"FamilySize\"] = pd.to_numeric(machine.validation_features[\"SibSp\"]) + pd.to_numeric(machine.validation_features[\"Parch\"]) + 1\n",
    "\n",
    "# machine.update_dtypes()\n",
    "\n",
    "\n",
    "# #################################################################################\n",
    "# ### feature transformation pipeline\n",
    "# # polynomial feature pipe\n",
    "# polynomial_pipe = PandasFeatureUnion([\n",
    "#     (\"polynomial\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "#         PandasTransformer(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "#     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"], exclude_columns=[\"Name\",\"Cabin\"]),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = polynomial_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = polynomial_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "\n",
    "\n",
    "# # encode & bin pipeline\n",
    "# encode_pipe = PandasFeatureUnion([\n",
    "#     (\"nominal\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"nominal\"]),\n",
    "#         PandasTransformer(OneHotEncoder(drop=\"first\")),\n",
    "#     )),\n",
    "#     (\"ordinal\", make_pipeline(\n",
    "#         DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "#         PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "#     )),\n",
    "# #     (\"bin\", make_pipeline(\n",
    "# #         DataFrameSelector(include_columns=machine.training_features.mlm_dtypes[\"continuous\"]),\n",
    "# #         PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "# #     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_columns=machine.training_features.mlm_dtypes[\"nominal\"] + list(ordinal_encodings.keys())),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = encode_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "\n",
    "\n",
    "# ###\n",
    "# # complex encoding\n",
    "# target_encode_pipe = PandasFeatureUnion([\n",
    "#     (\"target\", make_pipeline(\n",
    "#         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "#         KFoldEncoder(\n",
    "#             target=machine.training_target,\n",
    "#             cv=KFold(n_splits=5, shuffle=False),\n",
    "#             encoder=TargetEncoder,\n",
    "#         ),\n",
    "#     )),\n",
    "# #     (\"woe\", make_pipeline(\n",
    "# #         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "# #         KFoldEncoder(\n",
    "# #             target=machine.training_target,\n",
    "# #             cv=KFold(n_splits=5, shuffle=False),\n",
    "# #             encoder=WOEEncoder,\n",
    "# #         ),\n",
    "# #     )),\n",
    "# #     (\"catboost\", make_pipeline(\n",
    "# #         DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "# #         KFoldEncoder(\n",
    "# #             target=machine.training_target,\n",
    "# #             cv=KFold(n_splits=5, shuffle=False),\n",
    "# #             encoder=CatBoostEncoder,\n",
    "# #         ),\n",
    "# #     )),\n",
    "#     (\"diff\", make_pipeline(\n",
    "#         DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = target_encode_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = target_encode_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "\n",
    "\n",
    "# ### scale features\n",
    "# scale_pipe = PandasFeatureUnion([\n",
    "#     (\"scale\", make_pipeline(\n",
    "#         DataFrameSelector(),\n",
    "#         PandasTransformer(RobustScaler())\n",
    "#     )),\n",
    "# ])\n",
    "\n",
    "# machine.training_features = scale_pipe.fit_transform(machine.training_features)\n",
    "# machine.validation_features = scale_pipe.transform(machine.validation_features)\n",
    "\n",
    "# machine.update_dtypes()\n",
    "\n",
    "\n",
    "# # #################################################################################\n",
    "# # # remove outliers\n",
    "# # outliers = np.array([258, 305, 438, 679, 737, 745])\n",
    "# # machine.training_features = machine.training_features.drop(outliers)\n",
    "# # machine.training_target = machine.training_target.drop(index=outliers)\n",
    "\n",
    "# print('completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-29T21:34:06.491842Z",
     "start_time": "2020-02-29T21:34:06.017217Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# model/parameter space\n",
    "estimator_parameter_space = {\n",
    "    \"SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovo\", \"ovr\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.000000001, 5),\n",
    "    },\n",
    "    \"LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None, \"balanced\"]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
    "        # ,'boosting_type': hp.choice('boosting_type'\n",
    "        #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "        #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "        ,\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 20, 500),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.5),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 20000, 400000),\n",
    "    },\n",
    "#     \"LogisticRegression\": {\n",
    "#         \"C\": hp.loguniform(\"C\", np.log(0.001), np.log(0.2)),\n",
    "#         \"penalty\": hp.choice(\"penalty\", [\"l2\", 'none']),\n",
    "#     },\n",
    "    \"XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.3, 1),\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:50:39.577850Z",
     "start_time": "2020-02-02T04:30:41.545645Z"
    }
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "machine.exec_bayes_optim_search(\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    training_features=machine.training_features,\n",
    "    training_target=machine.training_target,\n",
    "    validation_features=machine.validation_features,\n",
    "    validation_target=machine.validation_target,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_folds=5,\n",
    "    n_jobs=2,\n",
    "    iters=125,\n",
    "    show_progressbar=True,\n",
    "    columns=fs.cross_val_features_dict\n",
    ")\n",
    "\n",
    "# save Machine object\n",
    "with open(os.path.join(machine.current_experiment_dir, \"machine\", \"machine.pkl\"), 'wb') as handle:\n",
    "    pickle.dump(machine, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.bayes_optim_summary.sort_values(\"validation_score\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model loss by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:51:08.269406Z",
     "start_time": "2020-02-02T04:51:04.562462Z"
    }
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(machine.bayes_optim_summary[\"estimator\"]):\n",
    "    machine.model_loss_plot(\n",
    "        bayes_optim_summary=machine.bayes_optim_summary,\n",
    "        estimator_class=estimator,\n",
    "        save_plots=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:51:32.716839Z",
     "start_time": "2020-02-02T04:51:17.771739Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(machine.bayes_optim_summary[\"estimator\"]):\n",
    "    machine.model_param_plot(\n",
    "        bayes_optim_summary=machine.bayes_optim_summary,\n",
    "        estimator_class=estimator,\n",
    "        estimator_parameter_space=estimator_parameter_space,\n",
    "        n_iter=1000,\n",
    "#         chart_scale=15,\n",
    "        title_scale=1.2,\n",
    "        save_plots=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T04:51:42.000322Z",
     "start_time": "2020-02-02T04:51:37.745333Z"
    }
   },
   "outputs": [],
   "source": [
    "# pair-wise comparison\n",
    "p = PrettierPlot(chart_scale=12)\n",
    "p.pair_plot_custom(\n",
    "    df=machine.unpack_bayes_optim_summary(machine.bayes_optim_summary, \"LGBMClassifier\"),\n",
    "    columns=[\"colsample_bytree\", \"learning_rate\", \"iteration\",\"iter_loss\"],\n",
    "    gradient_col=\"iteration\",\n",
    "    color=style.style_grey\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model performance evaluation - standard models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:36:02.620193Z",
     "start_time": "2020-02-02T05:36:02.584596Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "top_models = machine.top_bayes_optim_models(\n",
    "                bayes_optim_summary=machine.bayes_optim_summary,\n",
    "                metric=\"validation_score\",\n",
    "                num_models=1,\n",
    "            )\n",
    "top_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "# estimator_class = 'LGBMClassifier'; model_iter = 27\n",
    "# estimator_class = 'XGBClassifier'; model_iter = 148\n",
    "# estimator_class = 'RandomForestClassifier'; model_iter = 48\n",
    "# estimator_class = 'GradientBoostingClassifier'; model_iter = 402\n",
    "# estimator_class = 'AdaBoostClassifier'; model_iter = 418\n",
    "# estimator_class = 'ExtraTreesClassifier'; model_iter = 261\n",
    "estimator_class = 'SVC'; model_iter = 61\n",
    "# estimator_class = 'KNeighborsClassifier'; model_iter = 466\n",
    "\n",
    "model = machine.BayesOptimClassifierBuilder(\n",
    "    bayes_optim_summary=machine.bayes_optim_summary,\n",
    "    estimator_class=estimator_class,\n",
    "    model_iter=model_iter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-29T19:07:13.950389Z",
     "start_time": "2020-02-29T19:06:57.155400Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "machine.binary_classification_panel(\n",
    "    model=model,\n",
    "#     X_train=machine.training_features,\n",
    "#     y_train=machine.training_target,\n",
    "    X_train=machine.training_features,\n",
    "    y_train=machine.training_target,\n",
    "    X_valid=machine.validation_features,\n",
    "    y_valid=machine.validation_target,\n",
    "    labels=[\"Dies\", \"Survives\"],\n",
    "#     n_folds=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "machine.binary_prediction_summary(\n",
    "    model=model,\n",
    "    X_train=machine.training_features,\n",
    "    y_train=machine.training_target,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T02:20:39.749545Z",
     "start_time": "2019-10-28T02:19:28.468980Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = machine.BayesOptimClassifierBuilder(\n",
    "            bayes_optim_summary=machine.bayes_optim_summary,\n",
    "            estimator_class=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        machine.binary_classification_panel(\n",
    "            model=model,\n",
    "        #     X_train=machine.training_features,\n",
    "        #     y_train=machine.training_target,\n",
    "            X_train=machine.training_features,\n",
    "            y_train=machine.training_target,\n",
    "            X_valid=machine.validation_features,\n",
    "            y_valid=machine.validation_target,\n",
    "            labels=[\"Dies\", \"Survives\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Model explanability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:36:21.055671Z",
     "start_time": "2020-02-02T05:36:02.622778Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# estimator = 'LGBMClassifier'; model_iter = 21\n",
    "# estimator = 'XGBClassifier'; model_iter = 148\n",
    "# estimator = 'RandomForestClassifier'; model_iter = 493\n",
    "# estimator = 'GradientBoostingClassifier'; model_iter = 402\n",
    "# estimator = 'AdaBoostClassifier'; model_iter = 418\n",
    "# estimator = 'ExtraTreesClassifier'; model_iter = 261\n",
    "estimator = 'SVC'; model_iter = 61\n",
    "\n",
    "# estimator = 'KNeighborsClassifier'; model_iter = 466\n",
    "\n",
    "model = machine.BayesOptimClassifierBuilder(\n",
    "    bayes_optim_summary=machine.bayes_optim_summary,\n",
    "    estimator_class=estimator_class,\n",
    "    model_iter=model_iter,\n",
    ")\n",
    "\n",
    "model.fit(machine.training_features.values, machine.training_target.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:29:22.175054Z",
     "start_time": "2020-02-02T05:29:02.964809Z"
    }
   },
   "outputs": [],
   "source": [
    "machine.single_shap_viz_tree(obs_ix=444, model=model, data=machine.training_features, target=machine.training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:29:22.175054Z",
     "start_time": "2020-02-02T05:29:02.964809Z"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in machine.training_features.index[:5]:\n",
    "    machine.single_shap_viz_tree(obs_ix=i, model=model, data=machine.training_features, target=machine.training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:30:39.433662Z",
     "start_time": "2020-02-02T05:30:34.439195Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = machine.multi_shap_viz_tree(obs_ixs=machine.training_features.index, model=model, data=machine.training_features)\n",
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-02T05:36:25.308012Z",
     "start_time": "2020-02-02T05:36:21.058139Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = machine.multi_shap_value_tree(\n",
    "    obs_ixs=machine.training_features.index, model=model, data=machine.training_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "start_time": "2020-02-02T05:40:30.798Z"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "# grid_features = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "# grid_features = ['Age*Fare','Title_ordinal_encoded','Sex_male','Fare','Pclass_ordinal_encoded','CabinQuarter_X']\n",
    "grid_features = ['Age*Fare','Title_ordinal_encoded','Fare','Pclass_ordinal_encoded','Sex_male']\n",
    "\n",
    "\n",
    "machine.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=machine.training_features.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T02:41:14.815016Z",
     "start_time": "2019-10-28T02:41:14.583998Z"
    }
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "machine.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Fare\",\n",
    "    color_feature=\"Sex_male\",\n",
    "    feature_names=machine.training_features.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T02:42:25.004355Z",
     "start_time": "2019-10-28T02:42:22.854184Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = machine.training_features.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    machine.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Fare\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T02:42:41.951132Z",
     "start_time": "2019-10-28T02:42:41.645104Z"
    }
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "machine.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=machine.training_features.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plots for individual observations\n",
    "for i in machine.validation_features.index[:2]:\n",
    "    machine.single_shap_viz_tree(obsIx=i, model=model, data=machine.validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHAP force plot a set of data\n",
    "visual = machine.multi_shap_viz_tree(obs_ixs=machine.validation_features.index, model=model, data=machine.validation_features)\n",
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# generate SHAP values for set of observations\n",
    "obs_data, _, obs_shap_values = machine.multi_shap_value_tree(\n",
    "    obs_ixs=machine.validation_features.index, model=model, data=machine.validation_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plot grid\n",
    "grid_features = [\"Pclass\", \"Age\", \"Fare\", \"SibSp\",\"Parch\"]\n",
    "grid_features = [\"Pclass_ordinal_encoded\", \"Age\", \"Fare\"]\n",
    "\n",
    "\n",
    "machine.shap_dependence_grid(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    grid_features=grid_features,\n",
    "    all_features=machine.validation_features.columns,\n",
    "    dot_size=35,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# single SHAP dependence plot\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "\n",
    "machine.shap_dependence_plot(\n",
    "    obs_data=obs_data,\n",
    "    obs_shap_values=obs_shap_values,\n",
    "    scatter_feature=\"Age\",\n",
    "    color_feature=\"Parch\",\n",
    "    feature_names=machine.validation_features.columns,\n",
    "    dot_size=50,\n",
    "    alpha=0.5,\n",
    "    ax=ax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# SHAP dependence plots for all feature relative to an interaction feature\n",
    "feature_names = machine.validation_features.columns.tolist()\n",
    "top_shap = np.argsort(-np.sum(np.abs(obs_shap_values), 0))\n",
    "\n",
    "for top_ix in top_shap:\n",
    "    p = PrettierPlot()\n",
    "    ax = p.make_canvas()\n",
    "    \n",
    "    machine.shap_dependence_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        scatter_feature=feature_names[top_ix],\n",
    "        color_feature=\"Age\",\n",
    "        feature_names=feature_names,\n",
    "        dot_size=50,\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "machine.shap_summary_plot(\n",
    "        obs_data=obs_data,\n",
    "        obs_shap_values=obs_shap_values,\n",
    "        feature_names=machine.validation_features.columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Primary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "{'LGBMClassifier': [778],\n",
    " 'LogisticRegression': [730],\n",
    " 'XGBClassifier': [371],\n",
    " 'RandomForestClassifier': [712],\n",
    " 'GradientBoostingClassifier': [965],\n",
    " 'AdaBoostClassifier': [512],\n",
    " 'ExtraTreesClassifier': [244],\n",
    " 'SVC': [551],\n",
    " 'KNeighborsClassifier': [576]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-29T03:19:09.776612Z",
     "start_time": "2019-10-29T03:19:09.746614Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"LGBMClassifier\", model_iter=778)\n",
    "lr = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"LogisticRegression\", model_iter=730)\n",
    "xgb = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"XGBClassifier\", model_iter=371)\n",
    "rf = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"RandomForestClassifier\", model_iter=712)\n",
    "gb = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"GradientBoostingClassifier\", model_iter=965)\n",
    "ada = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"AdaBoostClassifier\", model_iter=512)\n",
    "ext = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"ExtraTreesClassifier\", model_iter=244)\n",
    "svc = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"SVC\", model_iter=551)\n",
    "kn = machine.BayesOptimClassifierBuilder(bayes_optim_summary=bayes_optim_summary, estimator=\"KNeighborsClassifier\", model_iter=576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-29T03:21:24.472308Z",
     "start_time": "2019-10-29T03:20:42.741085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vecstack import StackingTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Get your data\n",
    "\n",
    "# Initialize 1st level estimators\n",
    "estimators = [('lgb', lgb.model),\n",
    "              ('lr',lr.model),\n",
    "              ('xgb',xgb.model),\n",
    "              ('rf',rf.model),\n",
    "              ('gb',gb.model),\n",
    "              ('ada',ada.model),\n",
    "              ('ext',ext.model),\n",
    "              ('svc',svc.model),\n",
    "              ('kn',kn.model),\n",
    "             ]\n",
    "              \n",
    "# Initialize StackingTransformer\n",
    "stack = StackingTransformer(\n",
    "    estimators,\n",
    "    regression=False,\n",
    "    metric=accuracy_score,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit\n",
    "stack = stack.fit(machine.training_features, machine.training_target)\n",
    "\n",
    "# Get your stacked features\n",
    "oof_train = stack.transform(machine.training_features)\n",
    "oof_valid = stack.transform(machine.validation_features)\n",
    "\n",
    "# Use 2nd level estimator with stacked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T03:12:15.305201Z",
     "start_time": "2019-10-28T03:05:52.429813Z"
    }
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oof_train, oof_valid, columns = machine.model_stacker(\n",
    "    models=top_models,\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    X_train=machine.training_features.values,\n",
    "    y_train=machine.training_target.values,\n",
    "    X_valid=machine.validation_features.values,\n",
    "    n_folds=10,\n",
    "    n_jobs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-28T03:28:58.433646Z",
     "start_time": "2019-10-28T03:28:51.398069Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.make_canvas()\n",
    "p.corr_heatmap(\n",
    "    df=pd.DataFrame(oof_train, columns=columns), annot=True, ax=ax, vmin=0\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-29T03:22:06.292547Z",
     "start_time": "2019-10-29T03:22:06.264544Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "estimator_parameter_space = {\n",
    "    \"LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00000001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\", \"ovo\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-10-29T03:22:27.745685Z",
     "start_time": "2019-10-29T03:22:27.730686Z"
    }
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "machine.exec_bayes_optim_search(\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    data=oof_train,\n",
    "    target=machine.training_target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=8,\n",
    "    n_jobs=8,\n",
    "    iters=1000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "bayes_optim_summary_meta = pd.read_csv(\"{}_hyperopt_meta_{}.csv\".format(rundate, analysis))\n",
    "bayes_optim_summary_meta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# model loss plot\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    machine.model_loss_plot(bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "for estimator in np.unique(bayes_optim_summary_meta[\"estimator\"]):\n",
    "    machine.modelParamPlot(\n",
    "        bayes_optim_summary=bayes_optim_summary_meta,\n",
    "        estimator=estimator,\n",
    "        estimator_parameter_space=estimator_parameter_space,\n",
    "        n_iter=100,\n",
    "        chart_scale=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model performance evaluation - stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "top_models = machine.top_bayes_optim_models(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, num_models=1\n",
    ")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# classification panel, single model\n",
    "estimator = \"SVC\"; model_iter = 135\n",
    "# estimator = 'GradientBoostingClassifier'; model_iter = 590\n",
    "# estimator = 'XGBClassifier'; model_iter = 380\n",
    "\n",
    "model = machine.BayesOptimClassifierBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "machine.binary_classification_panel(\n",
    "    model=model, X_train=oof_train, y_train=machine.training_target, labels=[0, 1], n_folds=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# create classification reports for training data\n",
    "for estimator, model_iters in top_models.items():\n",
    "    for model_iter in model_iters:\n",
    "        model = machine.BayesOptimClassifierBuilder(\n",
    "            bayes_optim_summary=bayes_optim_summary_meta,\n",
    "            estimator=estimator,\n",
    "            model_iter=model_iter,\n",
    "        )\n",
    "        machine.binary_classification_panel(\n",
    "            model=model, X_train=oof_train, y_train=machine.training_target, labels=[0, 1], n_folds=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Submission - stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = \"LGBMClassifier\"; model_iter = 876 #0.75119\n",
    "# estimator = \"XGBClassifier\"; model_iter = 821, #0.779\n",
    "# estimator = \"RandomForestClassifier\"; model_iter = 82 \n",
    "# estimator = \"GradientBoostingClassifier\"; model_iter = 673 #0.77511\n",
    "estimator = \"SVC\"; model_iter = 538 # 0.77511\n",
    "\n",
    "# extract params and instantiate model\n",
    "model = machine.BayesOptimClassifierBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary_meta, estimator=estimator, model_iter=model_iter\n",
    ")\n",
    "\n",
    "model.fit(oof_train, machine.training_target.values)\n",
    "y_pred = model.predict(oof_valid)\n",
    "print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "submit = pd.DataFrame({\"PassengerId\": df_train.PassengerId, \"Survived\": y_pred})\n",
    "submit.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
